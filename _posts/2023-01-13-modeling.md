* 데이터 마이닝

* 머신러닝 : 매개변수를 자동으로 학습
* 지도학습 : 입력데이터 + 원하는 출력값
* 비지도학습 : 입력데이터

# 데이터분할

## 홀드아웃


```python
from sklearn.datasets import load_breast_cancer
breast_cancer = load_breast_cancer()
data = breast_cancer.data
target = breast_cancer.target
```


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,
                                                    target,
                                                    test_size = 0.3,
                                                    random_state = 2022,
                                                    stratify = target)    # target 컬럼의 0과 1의 비율을 반영하여 데이터를 분할
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

## K-fold


```python
import numpy as np
X = np.arange(10)
X
```


```python
from sklearn.model_selection import KFold
kfold = KFold(n_splits = 5)
```


```python
for train_idx, test_idx in kfold.split(X):
    print(train_idx, test_idx)
```


```python
import numpy as np
X = np.arange(15) 
y = [0] * 6 + [1] * 3 + [2] * 6 
y
```


```python
from sklearn.model_selection import StratifiedKFold
kfold = StratifiedKFold(n_splits = 3)
```


```python
for train_idx, test_idx in kfold.split(X, y) :    # y의 0,1,2 비율도 함께 고려
     print(train_idx, test_idx) 
```

## 교차검증

# 성과분석

## 분류지표



```python
# 혼동행렬
from sklearn.metrics import confusion_matrix

y_true = [0, 0, 0, 1, 1, 1]
y_pred = [0, 1, 0, 1, 1, 1]

confusion_matrix(y_true, y_pred)    # TN FP FN TP
```


```python
y_true = ['A', 'A', 'A', 'B', 'B', 'B']
y_pred = ['A', 'B', 'A', 'B', 'B', 'B']

confusion_matrix(y_true, y_pred, labels = ['A', 'B'])
```


```python
y_true = [0, 0, 0, 1, 1, 2, 2, 2, 2]
y_pred = [0, 1, 1, 1, 0, 0, 1, 2, 2]

confusion_matrix(y_true, y_pred) 
```


```python
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
y_true = [0, 0, 0, 1, 1, 1]
y_pred = [0, 1, 0, 1, 1, 1]

accuracy = accuracy_score(y_true, y_pred)
print(accuracy)

recall = recall_score(y_true, y_pred)
print(recall)

precision = precision_score(y_true, y_pred)
print(precision)

f1 = f1_score(y_true, y_pred)
print(f1)
```


```python
from sklearn.metrics import roc_curve, auc

y_true = [0, 0, 0, 1, 1, 1]
y_score = [0.1, 0.75, 0.35, 0.92, 0.81, 0.68]

fpr, tpr, thresholds = roc_curve(y_true, y_score)
AUC = auc(fpr, tpr)
print(AUC)

```

## 예측지표


```python
# MSE, MAE, MAPE
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import numpy as np
np.random.seed(123) # 난수 고정

y_true = np.random.random_sample(5) # 균일분포 (0,1)에서 5개 랜덤 추출
print(y_true)
y_pred = np.random.random_sample(5) 
print(y_pred)

mse = mean_squared_error(y_true, y_pred)
print(mse)
mae = mean_absolute_error(y_true, y_pred)
print(mae)
mape = mean_absolute_percentage_error(y_true, y_pred)
print(mape)
```


# 모델링

## 지도학습

### 로지스틱 회귀분석


```python
from sklearn.datasets import load_breast_cancer
breast_cancer = load_breast_cancer()
data = breast_cancer.data
target = breast_cancer.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,
                                                    target,
                                                    test_size = 0.2,
                                                    random_state = 2022,
                                                    stratify = target)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

from sklearn.linear_model import LogisticRegression
lr_bin = LogisticRegression(C = 0.5,     # 규제 강도
                   max_iter = 2000    # 수렴까지 걸리는 최대 반복 횟수
                  )
model_lr_bin = lr_bin.fit(X_train, y_train)

from sklearn.metrics import roc_curve, auc
y_score = model_lr_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score) 

print(AUC)
```


```python
from sklearn.datasets import load_iris
iris = load_iris() 
data = iris.data
target = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,
                                                    target,
                                                    test_size = 0.2,
                                                    random_state = 2205,
                                                    stratify = target)

from sklearn.linear_model import LogisticRegression
lr_multi = LogisticRegression(C = 0.05, # 규제 강도(default = 1.5)
                              max_iter = 200) # 수렴까지 걸리는 최대 반복 횟수


model_lr_multi = lr_multi.fit(X_train, y_train)

from sklearn.metrics import f1_score
y_pred = model_lr_multi.predict(X_test)

macro_f1 = f1_score(y_test, y_pred, average = "macro")
print(macro_f1)
```

### SVM

### 나이브베이즈

### K-NN

### 인공신경망

### 의사결정나무

### 앙상블 ☆☆☆☆☆

* breast_cancer 이진분류
* iris 다중분류
* diabetes 예측

#### 이진분류


```python
from sklearn.datasets import load_breast_cancer
breast_cancer = load_breast_cancer() 
data = breast_cancer.data
target = breast_cancer.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,
                                                    target,
                                                    test_size = 0.2,
                                                    random_state = 2022,    # 은근 random_state도 영향 많이 줌
                                                    stratify = target)


# 모형객체 생성 및 모델학습
# 배깅
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

dtr = DecisionTreeClassifier(max_depth=3,
                             min_samples_leaf=10)

bag_bin = BaggingClassifier(base_estimator=dtr,
                           n_estimators=500,
                           random_state=2022)
model_bag_bin = bag_bin.fit(X_train, y_train)

# 랜덤포레스트
from sklearn.ensemble import RandomForestClassifier
rf_bin = RandomForestClassifier(n_estimators = 500,
                                max_depth = 3,
                                min_samples_leaf = 10,
                                max_features = 'sqrt',
                                random_state = 2022)
model_rf_bin = rf_bin.fit(X_train, y_train)

# 에이다부스팅 AdaBoost
from sklearn.ensemble import AdaBoostClassifier
ada_bin = AdaBoostClassifier(n_estimators = 100,
                             learning_rate = 0.5,
                             random_state = 2022)
model_ada_bin = ada_bin.fit(X_train, y_train)

# 그레디언트 부스팅 GradientBoosting
from sklearn.ensemble import GradientBoostingClassifier
gbm_bin = GradientBoostingClassifier(n_estimators = 500,
                                     max_depth = 4,
                                     min_samples_leaf = 10,
                                     learning_rate = 0.1,
                                     random_state = 2022)
model_gbm_bin = gbm_bin.fit(X_train, y_train)

# XGBoost
from xgboost import XGBClassifier    # 사이킷런 래퍼(Wrapper) 패키지 
xgb_wrap_bin = XGBClassifier(max_depth = 8,
                             n_estimators = 500,
                             nthread = 5,
                             min_child_weight = 20,
                             gamma = 0.5,
                             objective = 'binary:logistic',
#                              use_label_encoder = False,
                             random_state = 2022)
model_xgb_wrap_bin = xgb_wrap_bin.fit(X_train, y_train)
#                                       , eval_metric = 'mlogloss')

# LightGBM
from lightgbm import LGBMClassifier    # 사이킷런 래퍼(Wrapper) 패키지 
lgb_wrap_bin = LGBMClassifier(max_depth = 8,
                              n_estimators = 500,
                              n_jobs = 30,
                              min_child_weight = 10,
                              learning_rate = 0.2,
                              objective = 'binary',
                              random_state = 2022)
model_lgb_wrap_bin = lgb_wrap_bin.fit(X_train, y_train)


# ROC 및 AUC
from sklearn.metrics import roc_curve, auc
y_score = model_bag_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score)
AUC = auc(fpr, tpr)    # roc_curve()에서 반환된 fpr을 x축, tpr을 y축
print('배깅 : ', format(AUC, '.4f'))

y_score = model_rf_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score) 
AUC = auc(fpr, tpr)
print('랜덤포레스트 : ', format(AUC, '.4f'))

y_score = model_ada_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score) 
AUC = auc(fpr, tpr)
print('에이다부스팅 : ', format(AUC, '.4f'))

y_score = model_gbm_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score) 
AUC = auc(fpr, tpr)
print('그레디언트부스팅 : ', format(AUC, '.4f'))

y_score = model_xgb_wrap_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score) 
AUC = auc(fpr, tpr) 
print('XGBoost : ', format(AUC, '.4f'))

y_score = model_lgb_wrap_bin.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_score) 
AUC = auc(fpr, tpr) 
print('LightGBM : ', format(AUC, '.4f'))
```

    배깅 :  0.9993
    랜덤포레스트 :  1.0000
    에이다부스팅 :  1.0000
    그레디언트부스팅 :  1.0000
    XGBoost :  0.9993
    LightGBM :  1.0000
    

#### 다중분류


```python
from sklearn.datasets import load_iris
iris = load_iris()
data = iris.data
target = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,
                                                    target,
                                                    test_size = 0.2,
                                                    random_state = 2222,
                                                    stratify = target)


# 모형객체 생성 및 모델학습
# 배깅
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

dtr = DecisionTreeClassifier(max_depth = 3,
                            min_samples_leaf = 10)
bag_multi = BaggingClassifier(base_estimator=dtr,
                              n_estimators=500,
                              random_state=2022)
model_bag_multi = bag_multi.fit(X_train, y_train)

# 랜덤포레스트
from sklearn.ensemble import RandomForestClassifier
rf_multi = RandomForestClassifier(n_estimators = 500,
                                  max_depth = 3,
                                  min_samples_leaf = 15,
                                  max_features = 'sqrt',
                                  random_state = 2022)
model_rf_multi = rf_multi.fit(X_train, y_train)

# 에이다부스팅 AdaBoost
from sklearn.ensemble import AdaBoostClassifier
ada_multi = AdaBoostClassifier(n_estimators = 500,
                               learning_rate = 0.01,
                               random_state = 2022)
model_ada_multi = ada_multi.fit(X_train, y_train)

# 그레디언트 부스팅 GradientBoosting
from sklearn.ensemble import GradientBoostingClassifier
gbm_multi = GradientBoostingClassifier(n_estimators = 500,
                                       max_depth = 8,
                                       min_samples_leaf = 5,
                                       learning_rate = 0.5,
                                       random_state = 2022)
model_gbm_multi = gbm_multi.fit(X_train, y_train)

# XGBoost
from xgboost import XGBClassifier
xgb_wrap_multi = XGBClassifier(max_depth = 8,
                               n_estimators = 500,
                               nthread = 5,
                               min_child_weight = 10,
                               gamma = 0.5,
                               objective = 'multi:softmax',
#                                use_label_encoder = False,
                               random_state = 2022)
model_xgb_wrap_multi = xgb_wrap_multi.fit(X_train, y_train)
#                                           , eval_metric = 'mlogloss')

# LightGBM
from lightgbm import LGBMClassifier
lgb_wrap_multi = LGBMClassifier(max_depth = 8,
                                n_estimators = 500,
                                n_jobs = 5,
                                min_child_weight = 10,
                                learning_rate = 0.5,
                                objective = 'multiclass',
                                random_state = 2022
                               )
model_lgb_wrap_multi = lgb_wrap_multi.fit(X_train, y_train)

# macro f1-score
from sklearn.metrics import f1_score
y_pred = model_bag_multi.predict(X_test)
macro_f1 = f1_score(y_test, y_pred, average='macro')
print('배깅 : ', format(macro_f1, '.4f'))

y_pred = model_rf_multi.predict(X_test)
macro_f1 = f1_score(y_test, y_pred, average='macro')
print('랜덤포레스트 : ', format(macro_f1, '.4f'))

y_pred = model_ada_multi.predict(X_test)
macro_f1 = f1_score(y_test, y_pred, average = "macro")
print('에이다부스팅 : ', format(macro_f1, '.4f'))

y_pred = model_gbm_multi.predict(X_test)
macro_f1 = f1_score(y_test, y_pred, average = "macro")
print('그레디언트부스팅 : ', format(macro_f1, '.4f'))

y_pred = model_xgb_wrap_multi.predict(X_test)
macro_f1 = f1_score(y_test, y_pred, average = "macro")
print('XGBoost : ', format(macro_f1, '.4f'))

y_pred = model_lgb_wrap_multi.predict(X_test)
macro_f1 = f1_score(y_test, y_pred, average = "macro")
print('LightGBM : ', format(macro_f1, '.4f'))

# 배깅 :  0.8329
# 랜덤포레스트 :  0.8997
# 에이다부스팅 :  0.8329
# 그레디언트부스팅 :  0.9327
# XGBoost :  0.8997
# LightGBM :  0.8667
```

    배깅 :  0.9666
    랜덤포레스트 :  1.0000
    에이다부스팅 :  0.9327
    그레디언트부스팅 :  0.8611
    XGBoost :  0.9666
    LightGBM :  0.9346
    


```python
y_train
```




    array([1, 0, 2, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 2,
           2, 0, 0, 2, 1, 1, 2, 2, 0, 0, 2, 0, 0, 2, 1, 1, 2, 2, 1, 0, 2, 0,
           0, 0, 0, 2, 0, 1, 1, 2, 1, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0, 1, 2,
           2, 2, 0, 2, 2, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 0, 1, 2, 0,
           1, 1, 2, 2, 0, 1, 1, 0, 1, 2, 2, 1, 0, 0, 1, 0, 2, 2, 0, 0, 2, 2,
           2, 1, 0, 0, 0, 1, 2, 2, 1, 1])



#### 회귀


```python
from sklearn.datasets import load_diabetes
diabetes = load_diabetes() 
data = diabetes.data
target = diabetes.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data,
                                                    target,
                                                    test_size = 0.2,
                                                    random_state = 2022)



# 모형객체 생성 및 모델학습
# 배깅
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor
dtr = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 10)
bag_conti = BaggingRegressor(base_estimator = dtr,
                              n_estimators = 500,
                              random_state = 2022)
model_bag_conti = bag_conti.fit(X_train, y_train)

# 랜덤포레스트
from sklearn.ensemble import RandomForestRegressor
rf_conti = RandomForestRegressor(n_estimators = 500,
                                 max_depth = 3,
                                 min_samples_leaf = 10,
                                 max_features = 3,
                                 random_state = 2022)
model_rf_conti = rf_conti.fit(X_train, y_train)

# 에이다부스팅 AdaBoost
from sklearn.ensemble import AdaBoostRegressor
ada_conti = AdaBoostRegressor(n_estimators = 500,
                              learning_rate = 0.01,
                              loss = 'square',
                              random_state = 2022)
model_ada_conti = ada_conti.fit(X_train, y_train)

# 그레디언트 부스팅 GradientBoosting
from sklearn.ensemble import GradientBoostingRegressor
gbm_conti = GradientBoostingRegressor(n_estimators = 500,
                                      max_depth = 2,
                                      min_samples_leaf = 5,
                                      learning_rate = 0.5,
                                      random_state = 2022)
model_gbm_conti = gbm_conti.fit(X_train, y_train)

# XGBoost
from xgboost import XGBRegressor
xgb_wrap_conti = XGBRegressor(max_depth = 8,
                              n_estimators = 500,
                              nthread = 5,
                              min_child_weight = 10,
                              gamma = 0.5,
                              objective = 'reg:squarederror',
                              use_label_encoder = False,
                              random_state = 2022)
model_xgb_wrap_conti = xgb_wrap_conti.fit(X_train, y_train)
#                                           ,eval_metric = 'rmse')

# LightGBM
from lightgbm import LGBMRegressor
lgb_wrap_conti = LGBMRegressor(max_depth = 8,
                               n_estimators = 500,
                               n_jobs = 5,
                               min_child_weight = 10,
                               learning_rate = 0.5,
                               objective = 'regression',
                               random_state = 2022)
model_lgb_wrap_conti = lgb_wrap_conti.fit(X_train, y_train)




# RMSE
from sklearn.metrics import mean_squared_error
y_pred = model_bag_conti.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared = False)
print('배깅 : ', format(rmse, '.4f'))

y_pred = model_rf_conti.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared = False)
print('랜덤포레스트 : ', format(rmse, '.4f'))

y_pred = model_ada_conti.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared = False)
print('에이다부스팅 : ', format(rmse, '.4f'))

y_pred = model_gbm_conti.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared = False)
print('그레디언트부스팅 : ', format(rmse, '.4f'))

y_pred = model_xgb_wrap_conti.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared = False)
print('XGBoost : ', format(rmse, '.4f'))

y_pred = model_lgb_wrap_conti.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared = False)
print('LightGBM : ', format(rmse, '.4f'))
```

    배깅 :  55.7056
    랜덤포레스트 :  56.1859
    에이다부스팅 :  57.1206
    그레디언트부스팅 :  69.6053
    XGBoost :  66.2925
    LightGBM :  76.5955
    

## 비지도학습 

### 군집 모형


```python

```
